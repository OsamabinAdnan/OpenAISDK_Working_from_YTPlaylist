{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Install openai_agents SDK**"
      ],
      "metadata": {
        "id": "-FTzPqIkCRKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM0ahwXqCDMn"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make your Jupyter Notebook capable of running asynchronous function using below code"
      ],
      "metadata": {
        "id": "GtisyugCCdwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "tMyTClT-Cmig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you are able to run async function in your Jupyter Notebook"
      ],
      "metadata": {
        "id": "-j2Nfiz_C83g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run Google Gemini with openAI agent SDK**"
      ],
      "metadata": {
        "id": "PmTdmTrODG9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "OmFJx0M8DDQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Check if the API key is present, if not then raise an error\n",
        "if not gemini_api_key:\n",
        "  raise ValueError(\"Gemini API key not found. Please set the GOOGLE_API_KEY environment variable.\")\n",
        "\n",
        "# Refernce: https://ai.google.dev/gemini-api/docs/openai\n",
        "# Below is the configuration by which you can connect third party LLM (Gemini in this case) with OpenAI SDK\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=external_client,\n",
        ")\n",
        "\n",
        "# Below configuration is on run level, other two configurations are Agent level and Global level\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True, # Tracing and observation part will be enable when it values set to False\n",
        ")\n"
      ],
      "metadata": {
        "id": "8IrVlIf4F0GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hello World Code | Method 01 | Sync Function**"
      ],
      "metadata": {
        "id": "AuCT0-XHO96d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent : Agent = Agent(name=\"Assistant\", instructions=\"You are helpful assistant\", model=model)\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello, who is the founder of Pakistan?\", run_config=config)\n",
        "\n",
        "print(\"\\nCALLING AGENT\\n\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGZ4eDJDPDVc",
        "outputId": "bf4950af-2c79-408f-9a57-a613453b467e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CALLING AGENT\n",
            "\n",
            "The founder of Pakistan is generally considered to be **Muhammad Ali Jinnah**. He was the leader of the All-India Muslim League and played a crucial role in advocating for the creation of a separate nation for Muslims in British India.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hello World Code | Method 02 | Async Function**"
      ],
      "metadata": {
        "id": "FvpP-m1xhZng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus\",\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(agent, \"Tell me about recursion in programming.\", run_config=config)\n",
        "  print(result.final_output)\n",
        "  # Function calls itself\n",
        "  # Looping in smaller pieces\n",
        "  # Endless by design.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XobqI83shfUV",
        "outputId": "bedc0eaa-3dcb-4b38-9ce0-6c108c1dd509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-calling routine,\n",
            "Breaks problem to smaller parts,\n",
            "Base case stops the call.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming possible due to Async function"
      ],
      "metadata": {
        "id": "hUXOGnvx87J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus\",\n",
        "  )\n",
        "\n",
        "  result = Runner.run_streamed(agent, \"Tell me about recursion in programming.\", run_config=config)\n",
        "  print(result)\n",
        "  async for e in result.stream_events():\n",
        "    print(e)\n",
        "  # Function calls itself\n",
        "  # Looping in smaller pieces\n",
        "  # Endless by design.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS82gm98BSrH",
        "outputId": "80bd6d45-667a-4157-cfbc-c8821f4ee781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunResultStreaming:\n",
            "- Current agent: Agent(name=\"Assistant\", ...)\n",
            "- Current turn: 0\n",
            "- Max turns: 10\n",
            "- Is complete: False\n",
            "- Final output (NoneType):\n",
            "    None\n",
            "- 0 new item(s)\n",
            "- 0 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResultStreaming` for more details)\n",
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions='You only respond in haikus', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1748371073.5108464, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='A', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' function calls itself', item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='A function calls itself', type='output_text'), sequence_number=5, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='A function calls itself', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, sequence_number=6, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1748371073.5108464, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='A function calls itself', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=7, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions='You only respond in haikus', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='A function calls itself', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    }
  ]
}